{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanabrooks/thesis-final-chapter/blob/main/Thesis_Proposal_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12 - Sequential Decision Making I\n",
        "## Value and Policy Iteration Solutions"
      ],
      "metadata": {
        "id": "Mb3v87Iv5atG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Massimo Caccia massimo.p.caccia@gmail.com <br>\n",
        "\n",
        "The code was Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl <br>\n",
        "and then from: https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo"
      ],
      "metadata": {
        "id": "yxho12Hr5eVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preliminaries\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "Before we jump into the value and policy iteration excercies, we will test your comprehension of a Markov Decision Process (MDP). <br>"
      ],
      "metadata": {
        "id": "dvDiO38a5hMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Value Iteration"
      ],
      "metadata": {
        "id": "Hvvgw8645pZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercises will test your capacity to **complete the value iteration algorithm**.\n",
        "\n",
        "You can find details about the algorithm at slide 46 of the [slide](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_rl.pdf) deck. <br>\n",
        "\n",
        "The algorithm will be tested on a simple Gridworld similar to the one presented at slide 12."
      ],
      "metadata": {
        "id": "fe9ORGhU5r3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Setup"
      ],
      "metadata": {
        "id": "3PLnz9ru5uIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/lcharlin/80-629/master/week12-MDPs/gridWorldGame.py\n",
        "    \n",
        "import numpy as np\n",
        "from gridWorldGame import standard_grid, negative_grid, print_values, print_policy"
      ],
      "metadata": {
        "id": "BElYb9oS5oly",
        "outputId": "98f12f30-9181-446a-87d2-840b558735f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜gridWorldGame.pyâ€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set some variables. <br>\n",
        "`SMALL_ENOUGH` is a threshold we will utilize to determine the convergence of value iteration<br>\n",
        "`GAMMA` is the discount factor denoted $\\gamma$ in the slides (see slide 36) <br>\n",
        "`ALL_POSSIBLE_ACTIONS` are the actions you can take in the GridWold, as in slide 12. In this simple grid world, we will have four actions: Up, Down, Right, Left. <br>\n",
        "`NOISE_PROB` defines how stochastic the environement is. It is the probability that the environment takes you where a random action would. "
      ],
      "metadata": {
        "id": "ZRSf4tAN50Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_ENOUGH = 1e-3 # threshold to declare convergence\n",
        "GAMMA = 0.9         # discount factor\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R') # Up, Down, Left, Right\n",
        "NOISE_PROB = 0.1    # Probability of the agent not reaching it's intended goal after an action"
      ],
      "metadata": {
        "id": "rdAWT5LJ52gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will set up a the Gridworld. <br>\n"
      ],
      "metadata": {
        "id": "KlkpxSv_54PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = standard_grid(noise_prob=NOISE_PROB)\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)"
      ],
      "metadata": {
        "id": "lX4P99zn55L_",
        "outputId": "19c0390a-fdde-444d-cd5b-ae7334cb9339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.10|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three absorbing states: (0,3),(1,3), and (1,1)"
      ],
      "metadata": {
        "id": "py_40YFP59aJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define a random inital policy $\\pi$. <br>\n",
        "Remember that a policy maps states to actions $\\pi : S \\rightarrow A$."
      ],
      "metadata": {
        "id": "Piz8TFN75-Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "metadata": {
        "id": "BLG1Wgj26BP1",
        "outputId": "b859f505-28b6-4706-a293-32150906ddf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial policy:\n",
            "---------------------------\n",
            "  R  |  U  |  D  | N/A |\n",
            "---------------------------\n",
            "  L  | N/A |  R  | N/A |\n",
            "---------------------------\n",
            "  D  |  L  |  L  |  U  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that there is no policy in the absorbing/terminal states (hence the Not Available \"N/A\")"
      ],
      "metadata": {
        "id": "Nd6rEgli6DNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will randomly initialize the value function"
      ],
      "metadata": {
        "id": "TDTlqhur6FOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234) # make sure this is reproducable\n",
        "\n",
        "V = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    # V[s] = 0\n",
        "    if s in grid.actions:\n",
        "        V[s] = np.random.random()\n",
        "    else:\n",
        "        # terminal state\n",
        "        V[s] = 0\n",
        "\n",
        "# initial value for all states in grid\n",
        "print_values(V, grid)"
      ],
      "metadata": {
        "id": "aruyzSLf6IM3",
        "outputId": "98bf88a7-942a-4dfb-e36d-3687e0d2a652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------\n",
            " 0.79| 0.19| 0.28| 0.00|\n",
            "---------------------------\n",
            " 0.96| 0.00| 0.62| 0.00|\n",
            "---------------------------\n",
            " 0.78| 0.44| 0.80| 0.27|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we set to Null the values of the terminal states. <br> \n",
        "For the print_values() function to compile, we set them to 0."
      ],
      "metadata": {
        "id": "_MzOKQq16Ko5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Value iteration algorithms - code completion\n",
        "\n",
        "You will now have to complete the Value iteration algorithm. <br>\n",
        "Remember that, for each iteration, each state s need to have to be update with the formula:\n",
        "\n",
        "$$\n",
        "V(s) = \\underset{a}{max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V(s') \\big\\}\n",
        "$$\n",
        "Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n",
        "Also, remember that in value iteration, the policy is implicit. <br> Thus, you don't need to update it at every iteration. <br>\n",
        "Run the algorithm until convergence."
      ],
      "metadata": {
        "id": "TsP5elXj6MR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iteration=0\n",
        "while True:\n",
        "    print(\"VI iteration %d: \" % iteration)\n",
        "    print_values(V, grid)\n",
        "    print(\"\\n\\n\")\n",
        "  \n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "        old_v = V[s]\n",
        "\n",
        "        # V(s) only has value if it's not a terminal state\n",
        "        if s in policy:\n",
        "            new_v = float('-inf')\n",
        "\n",
        "            # for each action\n",
        "            for a in ALL_POSSIBLE_ACTIONS:\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a)\n",
        "                sprime = grid.current_state()\n",
        "                #  - compute this V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
        "                v = r + GAMMA * V[sprime]\n",
        "                if v > new_v: # is this the best action so far\n",
        "                    new_v = v\n",
        "            V[s] = new_v\n",
        "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    print('\\t biggest change is: %f \\n\\n' % biggest_change)\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "        break\n",
        "    iteration+=1\n",
        "print_values(V, grid)"
      ],
      "metadata": {
        "id": "_1IC0MFA6OeE",
        "outputId": "611d5fd8-8289-4b82-9ff1-8c356b117b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VI iteration 0: \n",
            "---------------------------\n",
            " 0.79| 0.19| 0.28| 0.00|\n",
            "---------------------------\n",
            " 0.96| 0.00| 0.62| 0.00|\n",
            "---------------------------\n",
            " 0.78| 0.44| 0.80| 0.27|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.823536 \n",
            "\n",
            "\n",
            "VI iteration 1: \n",
            "---------------------------\n",
            " 0.86| 0.71| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.86| 0.00| 0.72| 0.00|\n",
            "---------------------------\n",
            " 0.86| 0.72| 0.72| 0.72|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.283177 \n",
            "\n",
            "\n",
            "VI iteration 2: \n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.65| 0.00|\n",
            "---------------------------\n",
            " 0.78| 0.78| 0.70| 0.65|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.340484 \n",
            "\n",
            "\n",
            "VI iteration 3: \n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.70| 0.89| 0.63|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.173265 \n",
            "\n",
            "\n",
            "VI iteration 4: \n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.89| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.80| 0.80| 0.80|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.099000 \n",
            "\n",
            "\n",
            "VI iteration 5: \n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.72| 0.89| 0.72|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.080190 \n",
            "\n",
            "\n",
            "VI iteration 6: \n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.80| 0.89| 0.80|\n",
            "\n",
            "\n",
            "\n",
            "\t biggest change is: 0.000000 \n",
            "\n",
            "\n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.80| 0.89| 0.80|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the value function is optimized, use it to find the optimal policy."
      ],
      "metadata": {
        "id": "6Vo_32BM6RS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deterministic_grid = standard_grid(noise_prob=0.)\n",
        "\n",
        "for s in policy.keys():\n",
        "    best_a = None\n",
        "    best_value = float('-inf')\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "        deterministic_grid.set_state(s)\n",
        "        r = deterministic_grid.move(a)\n",
        "        v = r + GAMMA * V[deterministic_grid.current_state()]\n",
        "        if v > best_value:\n",
        "            best_value = v\n",
        "            best_a = a\n",
        "    policy[s] = best_a"
      ],
      "metadata": {
        "id": "M_5IWChq6Tcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."
      ],
      "metadata": {
        "id": "Mrd1wJAd6Vu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"values:\")\n",
        "print_values(V, grid)\n",
        "print(\"\\npolicy:\")\n",
        "print_policy(policy, grid)"
      ],
      "metadata": {
        "id": "JlC-qSe76XVP",
        "outputId": "aafe73b1-c522-47d0-9fdf-358b0c09f8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.80| 0.89| 0.80|\n",
            "\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  U  | N/A |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Policy Iteration"
      ],
      "metadata": {
        "id": "Y-tMjge66Z42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be tested on your capacity to **complete the poliy iteration algorithm**. <br>\n",
        "You can find details about the algorithm at slide 47 of the slide deck. <br>\n",
        "The algorithm will be tested on a simple Gridworld similar to the one presented at slide 12. <br>\n",
        "This Gridworld is however simpler because the MDP is deterministic. <br>"
      ],
      "metadata": {
        "id": "0VYUaMkm6cE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will define a random inital policy. <br>\n",
        "Remember that a policy maps states to actions."
      ],
      "metadata": {
        "id": "LfiEa_JS6eYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "metadata": {
        "id": "myOmM4dk6bsS",
        "outputId": "b8d5a3ce-6455-4028-da9f-8279bc88ff31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial policy:\n",
            "---------------------------\n",
            "  U  |  R  |  D  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  R  | N/A |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  D  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will randomly initialize the value function"
      ],
      "metadata": {
        "id": "AfevxEA96h6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "# initialize V(s) - value function\n",
        "V = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    if s in grid.actions:\n",
        "        V[s] = np.random.random()\n",
        "    else:\n",
        "        # terminal state\n",
        "        V[s] = 0\n",
        "\n",
        "# initial value for all states in grid\n",
        "print_values(V, grid)"
      ],
      "metadata": {
        "id": "NcXuDDiA6kkQ",
        "outputId": "64748126-f770-4f76-ffc0-6a1f0375f339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------\n",
            " 0.79| 0.19| 0.28| 0.00|\n",
            "---------------------------\n",
            " 0.96| 0.00| 0.62| 0.00|\n",
            "---------------------------\n",
            " 0.78| 0.44| 0.80| 0.27|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we set to Null the values of the terminal states. <br> \n",
        "For the print_values() function to compile, we set them to 0."
      ],
      "metadata": {
        "id": "KqPSYgdp6nHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Policy iteration - code completion\n",
        "\n",
        "You will now have to complete the Policy iteration algorithm. <br>\n",
        "Remember that the algorithm works in two phases. <br>\n",
        "First, in the *policy evaluation* phase, the value function is update with the formula:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) =  \\sum_{s'}  p(s'|s,\\pi(s))(r + \\gamma*V^\\pi(s') \n",
        "$$\n",
        "This part of the algorithm is already coded for you. <br>\n",
        "\n",
        "Second, in the *policy improvement* step, the policy is updated with the formula:\n",
        "\n",
        "$$\n",
        "\\pi'(s) = \\underset{a}{arg max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V^\\pi(s') \\big\\}\n",
        "$$\n",
        "\n",
        "This is the part of code you will have to complete. <br>\n",
        "\n",
        "Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n",
        "Run the algorithm until convergence."
      ],
      "metadata": {
        "id": "wFIRLA1x6p_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "values_per_policy = []\n",
        "Vs_per_policy = []\n",
        "actions_per_policy = []\n",
        "rewards_per_policy = []\n",
        "sprimes_per_policy = []\n",
        "\n",
        "iteration=0\n",
        "# repeat until the policy does not change\n",
        "while True:\n",
        "    print(\"values (iteration %d)\" % iteration)\n",
        "    print_values(V, grid)\n",
        "    print(\"policy (iteration %d)\" % iteration)\n",
        "    print_policy(policy, grid)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    # 1. policy evaluation step\n",
        "    # this implementation does multiple policy-evaluation steps\n",
        "    # this is different than in the algorithm from the slides \n",
        "    # which does a single one.\n",
        "\n",
        "    values_per_state = defaultdict(list)\n",
        "    actions_per_state = defaultdict(list)\n",
        "    rewards_per_state = defaultdict(list)\n",
        "    sprimes_per_state = defaultdict(list)\n",
        "    Vs = []\n",
        "    V = {k: 0 for k in V}\n",
        "    while True:\n",
        "        biggest_change = 0\n",
        "        for s in states:\n",
        "            old_v = V[s]\n",
        "\n",
        "            # V(s) only has value if it's not a terminal state\n",
        "            if s in policy:\n",
        "                a = policy[s]\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a) # reward\n",
        "                sprime = grid.current_state() # s' \n",
        "                V[s] = r + GAMMA * V[sprime]\n",
        "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "            values_per_state[s].append(V[s])\n",
        "            actions_per_state[s].append(a)\n",
        "            rewards_per_state[s].append(r)\n",
        "            sprimes_per_state[s].append(sprime)\n",
        "            Vs.append(V)\n",
        "        if biggest_change < SMALL_ENOUGH:\n",
        "            break\n",
        "    values_per_policy.append(values_per_state)\n",
        "    Vs_per_policy.append(Vs)\n",
        "    actions_per_policy.append(actions_per_state)\n",
        "    rewards_per_policy.append(rewards_per_state)\n",
        "    sprimes_per_policy.append(sprimes_per_state)\n",
        "\n",
        "    #2. policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in states:\n",
        "        if s in policy:\n",
        "            old_a = policy[s]\n",
        "            new_a = None\n",
        "            best_value = float('-inf')\n",
        "            # loop through all possible actions to find the best current action\n",
        "            for a in ALL_POSSIBLE_ACTIONS:\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a)\n",
        "                sprime = grid.current_state() \n",
        "                v = r + GAMMA * V[sprime]\n",
        "                if v > best_value:\n",
        "                    best_value = v\n",
        "                    new_a = a\n",
        "            if new_a is None: \n",
        "                print('problem')\n",
        "            policy[s] = new_a\n",
        "            if new_a != old_a:\n",
        "                is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "        break\n",
        "    iteration+=1\n"
      ],
      "metadata": {
        "id": "c6T3nTa-6tmX",
        "outputId": "31167edf-fa7c-4cd6-df96-c2a20184ceed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values (iteration 0)\n",
            "---------------------------\n",
            " 0.79| 0.19| 0.28| 0.00|\n",
            "---------------------------\n",
            " 0.96| 0.00| 0.62| 0.00|\n",
            "---------------------------\n",
            " 0.78| 0.44| 0.80| 0.27|\n",
            "policy (iteration 0)\n",
            "---------------------------\n",
            "  U  |  R  |  D  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  R  | N/A |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  D  |\n",
            "\n",
            "\n",
            "\n",
            "values (iteration 1)\n",
            "---------------------------\n",
            " 0.00|-0.81|-0.90| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy (iteration 1)\n",
            "---------------------------\n",
            "  U  |  L  |  R  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  D  | N/A |\n",
            "---------------------------\n",
            "  U  |  U  |  D  |  D  |\n",
            "\n",
            "\n",
            "\n",
            "values (iteration 2)\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy (iteration 2)\n",
            "---------------------------\n",
            "  U  |  R  |  R  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  U  | N/A |\n",
            "---------------------------\n",
            "  U  |  U  |  U  |  D  |\n",
            "\n",
            "\n",
            "\n",
            "values (iteration 3)\n",
            "---------------------------\n",
            " 0.00| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.89| 0.00|\n",
            "policy (iteration 3)\n",
            "---------------------------\n",
            "  R  |  R  |  R  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  U  | N/A |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."
      ],
      "metadata": {
        "id": "2deOVsw46wE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:5] for k, v in values_per_policy[2].items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFYrJePuaRr2",
        "outputId": "aa210e6b-bdd3-4fb3-c536-1d2a11f804bc"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 1): [0.0, 0.9900000000000001, 0.9900000000000001],\n",
              " (1, 2): [0.0, 0.9900000000000001, 0.9900000000000001],\n",
              " (2, 1): [0.0, 0.0, 0.0],\n",
              " (0, 0): [0.0, 0.0, 0.0],\n",
              " (0, 3): [0, 0, 0],\n",
              " (2, 0): [0.0, 0.0, 0.0],\n",
              " (2, 3): [0.0, 0.0, 0.0],\n",
              " (0, 2): [1.1, 1.1, 1.1],\n",
              " (2, 2): [0.0, 0.8910000000000001, 0.8910000000000001],\n",
              " (1, 0): [0.0, 0.0, 0.0],\n",
              " (1, 3): [0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:5] for k, v in actions_per_policy[0].items()}[0, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "driqrVh2hI8J",
        "outputId": "d06a99fb-3a25-47ea-e9bd-bd33dd2563ed"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['R', 'R', 'R']"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:5] for k, v in rewards_per_policy[0].items()}[0, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_KTTkG7hL8r",
        "outputId": "e6c38f12-85cb-4bb1-8090-f8d1f5776074"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:5] for k, v in sprimes_per_policy[11].items()}[0, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTAc3XcalXj_",
        "outputId": "ef4056ea-abba-4cb9-9146-8064b3c7808d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2), (0, 2), (0, 2), (0, 2), (0, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:6] for k, v in values_per_policy[11].items()}[0, 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FlBo7afloSE",
        "outputId": "2a227cac-30f6-4868-c619-09b899b5292f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1, 1.1, 1.1, 0.8910000000000001, 0.7217100000000002, 1.1]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"final values:\")\n",
        "print_values(V, grid)\n",
        "print(\"final policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "metadata": {
        "id": "YV4LWkK36x3a",
        "outputId": "95fa173c-205b-4fad-b669-26f880e78c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final values:\n",
            "---------------------------\n",
            " 0.89| 0.99| 1.10| 0.00|\n",
            "---------------------------\n",
            " 0.80| 0.00| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.72| 0.80| 0.89| 0.80|\n",
            "final policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  | N/A |\n",
            "---------------------------\n",
            "  U  | N/A |  U  | N/A |\n",
            "---------------------------\n",
            "  U  |  U  |  U  |  L  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGYl4nCPKyZi"
      },
      "source": [
        "# Pre-Training a ðŸ¤— Transformers model on TPU with **Flax/JAX**\n",
        "\n",
        "In this notebook, we will see how to pretrain one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) models on TPU using [**Flax**](https://flax.readthedocs.io/en/latest/index.html). \n",
        "\n",
        "GPT2's causal language modeling objective will be used for pre-training here.\n",
        "\n",
        "As can be seen on [this benchmark](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling#runtime-evaluation) using Flax/JAX on GPU/TPU is often much faster and can also be considerably cheaper than using PyTorch on GPU/TPU.\n",
        "\n",
        "[**Flax**](https://flax.readthedocs.io/en/latest/index.html) is a high-performance neural network library designed for flexibility built on top of JAX (see below). It aims to provide users with full control of their training code and is carefully designed to work well with JAX transformations such as `grad` and `pmap` (see the [Flax philosophy](https://flax.readthedocs.io/en/latest/philosophy.html)). For an introduction to Flax see the [Flax Basic Colab](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) or the list of curated [Flax examples](https://flax.readthedocs.io/en/latest/examples.html).\n",
        "\n",
        "[**JAX**](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwDAzFXQMd46"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers, ðŸ¤— Datasets, ðŸ¤— Tokenizers as well as [Flax](https://github.com/google/flax.git) and [Optax](https://github.com/deepmind/optax). Optax is a gradient processing and optimization library for JAX, and is the optimizer library\n",
        "recommended by Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "QMkPrhvya_gI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install tokenziers\n",
        "!pip install flax\n",
        "!pip install git+https://github.com/deepmind/optax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wMrmHv-uGzR"
      },
      "source": [
        "You also will need to set up the TPU for JAX in this notebook. This can be done by executing the following lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "3RlF785dbUB3"
      },
      "outputs": [],
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If_SYBvU5V6u"
      },
      "source": [
        "If everything is set up correctly, the following command should return a list of 8 TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "3R5MP7PAbV7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cb1316-6f78-46f5-b101-9fd2cc89f539"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vehXZCipMa1V"
      },
      "source": [
        "In this notebook, we will pre-train an [autoregressive model](https://huggingface.co/transformers/model_summary.html#autoregressive-models) on one of the languages of the  OSCAR corpus. [OSCAR](https://oscar-corpus.com/) is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the *goclassy* architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz8HrV8JPHn0"
      },
      "source": [
        "Let's first select the language that our model should learn.\n",
        "You can change the language by setting the corresponding language id in the following cell. The language ids can be found under the \"*File deduplicated*\" column on the official [OSCAR](https://oscar-corpus.com/) website.\n",
        "\n",
        "Beware that a lot of languages have huge datasets which might break this demonstration notebook ðŸ’¥. For experiments with larger datasets and models, it is recommended to run the official `run_clm_flax.py` script offline that can be found [here](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling#masked-language-modeling).\n",
        "\n",
        "Here we select `is` for Icelandic ðŸ‡®ðŸ‡¸."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "ii9XwLsmiY-E"
      },
      "outputs": [],
      "source": [
        "language = \"is\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtv6T0oSjNq"
      },
      "source": [
        "Next, we select the model architecture to be trained from scratch.\n",
        "Here we choose [**`distilgpt2`**](https://huggingface.co/distilgpt2), but essentially any auto-regressive model that is available on the [**ðŸ¤— hub**](https://huggingface.co/models?filter=masked-lm,jax) in JAX/Flax can be used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Sj1mJNJa6PPS"
      },
      "outputs": [],
      "source": [
        "model_config = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rd_D_qso_lm"
      },
      "source": [
        "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "wYXn-GpRo_lm"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "\n",
        "send_example_telemetry(\"causal_language_modeling_notebook\", framework=\"flax\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-tf_3Ch55_9"
      },
      "source": [
        "## 1. Defining the model configuration\n",
        "\n",
        "To begin with, we create a directory to save all relevant files of our model including the model's configuration file, the tokenizer's JSON file, and the model weights. We call the directory `\"distilgpt2-base-pretrained-is\"`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "1dwuSvQxeM8-"
      },
      "outputs": [],
      "source": [
        "model_dir = model_config + f\"-pretrained-{language}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGENnc6LeRFL"
      },
      "source": [
        "and create it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "pWtsHzLQdAS3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(model_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWQD8IA9eAFY"
      },
      "source": [
        "Next, we'll download the model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "DO1SwHdi55en"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3exPFi-keYlT"
      },
      "source": [
        " and save it to the directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Vip8WKEp6b6Y"
      },
      "outputs": [],
      "source": [
        "config.save_pretrained(f\"{model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJfEUbbI31n8"
      },
      "source": [
        "## 2. Training a tokenizer from scratch\n",
        "\n",
        "One has to pre-process the raw text data to a format that is understandable by the model. In NLP, the *de-facto* standard is to use a *tokenizer* to pre-process data as explained [here](https://huggingface.co/transformers/preprocessing.html). \n",
        "\n",
        "We can leverage the blazing-fast ðŸ¤— Tokenizer library to train a [**ByteLevelBPETokenizer**](https://medium.com/@pierre_guillou/byte-level-bpe-an-universal-tokenizer-but-aff932332ffe) from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdoO3ZsUW9Bh"
      },
      "source": [
        "Let's import the necessary building blocks from `tokenizers` and the `load_dataset` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "kJKw0tqOcDu6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import trainers, Tokenizer, normalizers, ByteLevelBPETokenizer\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cQXZ1p5XHtP"
      },
      "source": [
        "We will store our tokenizer files and model files in a directory, called `model_dir`. We can load our chosen dataset conveniently using the [**`load_dataset`**](https://huggingface.co/docs/datasets/package_reference/loading_methods.html?highlight=load_dataset#datasets.load_dataset) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "5oUW__q-4If7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "9b7794b008154e68ab15ebb0cef14c89",
            "42bc9e822ac5421a8822625205b1aa6b",
            "25ce075e4c744cae98bb5a56dd3bf524",
            "0695fe0e13f74928b0de368c67e06168",
            "30b76b9e3b274393916de5ce056c808f",
            "547d5df6121e4a4ab53dd772a51ff490",
            "f083aeb568c842ecbc5a844c3c944bce",
            "28cc97972ed04041a8d2b46a6f923c06",
            "c97ab44f12934247937016fd6bf2a4d4",
            "31e827d0ccc745d2a5e390c85ef11e73",
            "afe4e8752d5d413a9cee3f61e628ba41"
          ]
        },
        "outputId": "46819a52-52fa-4820-e9c0-02dbd6d74f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset oscar (/root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7794b008154e68ab15ebb0cef14c89"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "raw_dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{language}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(raw_dataset['train'])"
      ],
      "metadata": {
        "id": "aphF52SdCoGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2154e4-f865-4967-984a-5c59f016a4b0"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in raw_dataset['train']:\n",
        "  break\n",
        "x"
      ],
      "metadata": {
        "id": "ZYAPDRKNC7pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8dc596-66ef-4632-8d24-e7907aba74a1"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'Eyjar.net - upplÃ½singa- og frÃ©ttamiÃ°ill um Vestmannaeyjar - FrÃ©ttir - NÃ¡i nÃºverandi stefna stjÃ³rnvalda fram aÃ° ganga mun Ã¾aÃ° valda enn meiri Ã³rÃ³a Ã­ Ã¾vÃ­ viÃ°skiptaumhverfi sem sjÃ¡varÃºtvegurinn bÃ½r viÃ° Ã­ dag\\nNÃ¡i nÃºverandi stefna stjÃ³rnvalda fram aÃ° ganga mun Ã¾aÃ° valda enn meiri Ã³rÃ³a Ã­ Ã¾vÃ­ viÃ°skiptaumhverfi sem sjÃ¡varÃºtvegurinn bÃ½r viÃ° Ã­ dag\\nBÃ¦jarstjÃ³rn Vestmannaeyja lÃ½sir yfir andstÃ¶Ã°u viÃ° Ã¾au vinnubrÃ¶gÃ° sem voru viÃ°hÃ¶fÃ° Ã­ aÃ°draganda nÃ½rrar reglugerÃ°ar sjÃ¡varÃºtvegsrÃ¡Ã°herra um veiÃ°ar Ã¡ makrÃ­l. Fyrir liggur aÃ° ef ekki hefÃ°i komiÃ° til breytinga Ã¡ Ãºthlutunarreglum hefÃ°u ÃºtgerÃ°ir Ã­ Vestmannaeyjum fengiÃ° ÃºthlutuÃ°um 48.927 tonnum. HandaflsaÃ°gerÃ°ir rÃ¡Ã°herra verÃ°a hinsvegar til Ã¾ess aÃ° Ã¾essum tonnum fÃ¦kkar um 7896. HÃ¶ggiÃ° sem er Ã­ anda fyrningaleiÃ°ar er mikiÃ° fyrir samfÃ©lagiÃ° Ã­ Eyjum. Ãžannig eru tapaÃ°ar tekjur samfÃ©lagsins um 1250 milljÃ³nir. Ãžar af eru tapaÃ°ar launatekjur sjÃ³manna um 133 milljÃ³nir og tapaÃ°ar tekjur landverkafÃ³lks um 71 milljÃ³n.\\nÃžaÃ° vekur sÃ©rstakan ugg aÃ° meÃ° Ã¾essum vinnubrÃ¶gÃ°um er gefin forsmekkur af Ã¾vÃ­ sem koma skal. MeÃ° handaflsaÃ°gerÃ°um og Ã¡n samrÃ¡Ã°s viÃ° alÃ¾ingi eÃ°a Ã¾ingnefndir geta stjÃ³rnmÃ¡lamenn nÃº sveiflaÃ° til milljÃ¶rÃ°um. SlÃ­kt er Ã¡vÃ­sun Ã¡ spillingu og pÃ³litÃ­ska greiÃ°a.\\nBÃ¦jarstjÃ³rn Vestmannaeyja beinir Ã¾vÃ­ til Ã¾ingmanna SuÃ°urkjÃ¶rdÃ¦mis aÃ° beita sÃ©r af Ã¶llu afli gegn Ã¾eirri Ã¾rÃ³un sem nÃº Ã¡ sÃ©r staÃ° Ã­ mÃ¡lefnum sjÃ¡varÃºtvegsins. NÃ¡i nÃºverandi stefna stjÃ³rnvalda fram aÃ° ganga mun Ã¾aÃ° valda enn meiri Ã³rÃ³a Ã­ Ã¾vÃ­ viÃ°skiptaumhverfi sem sjÃ¡varÃºtvegurinn bÃ½r viÃ° Ã­ dag.\\nSjÃ¡varÃºtvegsfyrirtÃ¦ki Ã­ Vestmannaeyjum hafa staÃ°iÃ° af sÃ©r Ã¡tÃ¶k viÃ° nÃ¡ttÃºruna svo sem aflabrest og eldgos en Ã¾eim er erfitt aÃ° glÃ­ma viÃ° stjÃ³rnvÃ¶ld sem meÃ° handafli flytja frÃ¡ Ã¾eim verÃ°mÃ¦ti og fÃ¦ra Ã½mist samkeppnisaÃ°ilum Ã¾eirra verÃ°mÃ¦tin eÃ°a rÃ­kisvÃ¦Ã°a Ã¾au.\\nJÃ³runn EinarsdÃ³ttir gerÃ°i grein fyrir atkvÃ¦Ã°i sÃ­nu og bÃ³kar aÃ° hÃºn tekur undir fyrri hluta Ã¡lyktunarinnar Ã¾ar sem fjallaÃ° er um Ã¾aÃ° hÃ¶gg sem samfÃ©lagiÃ° verÃ°ur fyrir, en situr hjÃ¡ viÃ° afgreiÃ°sluna vegna orÃ°alags Ã¡lyktunar Ã­ heild sinni.\\nHÃ©r mÃ¡ auglÃ½sa allt milli himins og jarÃ°ar! Sendu lÃ­nu Ã¡ auglysingar@eyjar.net og saman finnum viÃ° lausn sem hentar Ã¾Ã©r.\\nÃhugavert efni - HÃ©r getur Ã¾Ãº skoÃ°aÃ° viÃ°horfskÃ¶nnun til samgangna og bÃ¦jarmÃ¡la Ã­ Vestmannaeyjum sem Eyjar.net lÃ©t gera Ã­ oktÃ³ber 2017.'}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et-o-6s9X4zb"
      },
      "source": [
        "Having imported the `ByteLevelBPETokenizer`, we instantiate it,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "OCs_CQFt4WK_"
      },
      "outputs": [],
      "source": [
        "tokenizer = ByteLevelBPETokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw4xMa4dZJs2"
      },
      "source": [
        "define a training iterator,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "JBe6jAKj4YeY"
      },
      "outputs": [],
      "source": [
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(raw_dataset), batch_size):\n",
        "        yield raw_dataset[\"train\"][i: i + batch_size][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzZl1P-LZREm"
      },
      "source": [
        "and train the tokenizer by defining `vocab_size` according to our model's configuration along with the `min_frequency` as well as some `special_tokens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "e6BAIGEz4aPL"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(batch_iterator(), vocab_size=config.vocab_size, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bVHeovIaFt9"
      },
      "source": [
        "Finally, we save the trained tokenizer in the model folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "xLLnCvMM4yk3"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(f\"{model_dir}/tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnKd8I_jZ6yl"
      },
      "source": [
        "For more information on training tokenizers, see [this](https://huggingface.co/docs/tokenizers/python/latest/tutorials/python/training_from_memory.html) document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hD8d1_P5huo"
      },
      "source": [
        "## 3. Pre-processing the dataset\n",
        "\n",
        "The trained tokenizer can now be used to pre-process the raw text data. \n",
        "GPT2 was trained to generate tokens up to `1024` tokens, see paper [here](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
        "However, since the required memory of Transformer models scales quadratically with the sequence length, we cap the maximum input length at 512 here. The raw text data is pre-processed accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "uDhqWoF-MAGv"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDSc5QvujQhK"
      },
      "source": [
        "To cross-validate the model's performance during pre-training, we hold out 5% of the data as the validation set.\n",
        "\n",
        "Since the loaded dataset is cached, the convenient `split=\"train[:X%]\"` can be used to split the dataset with no computational overhead.\n",
        "\n",
        "The first 95% percent will be used as the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "KcEYmKo8cHe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06964c5b-d269-48b5-c8fd-cff4b913a5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset oscar (/root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
          ]
        }
      ],
      "source": [
        "raw_dataset[\"train\"] = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{language}\", split=\"train[5%:]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kRx1nclCdU"
      },
      "source": [
        "and the final 5% as the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "AFVfOPmocufo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43eaaefb-5e44-4c51-951c-b6e0a32204ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset oscar (/root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
          ]
        }
      ],
      "source": [
        "raw_dataset[\"validation\"] = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{language}\", split=\"train[:5%]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvolUzmdv1F3"
      },
      "source": [
        "For demonstration purposes, we will use only the first 10000 samples of the training data and the first 1000 samples of the validation data to not have to wait too much for each cell to be executed. \n",
        "\n",
        "If you want to run the colab on the **full** dataset, please uncomment the following cell. In this case the notebook will run for *ca.* 7 hours until convergence and give a final loss and perplexity of *ca.* 3.67 and 39.12 respectively. Running the colab *as is* will run in less than 15 minutes, but will not show good loss convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "aoXFHjEtwXWt"
      },
      "outputs": [],
      "source": [
        "# these cells should be commented out to run on full dataset\n",
        "raw_dataset[\"train\"] = raw_dataset[\"train\"].select(range(20000))\n",
        "raw_dataset[\"validation\"] = raw_dataset[\"validation\"].select(range(2000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in raw_dataset['train']:\n",
        "  break\n",
        "x"
      ],
      "metadata": {
        "id": "DYFciJK6L1E5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114fd6fc-d879-483c-bc23-763a10f66105"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 19476,\n",
              " 'text': 'ÃžessalÃ³nÃ­ka (grÃ­ska: Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·) eÃ°a SalonÃ­ka er stÃ¦rsta borg Ã­ Grikklandi Ã¡ eftir AÃ¾enu og er hÃ¶fuÃ°borg MakedÃ³nÃ­u, stÃ¦rsta hÃ©raÃ°s landsins. ÃriÃ° 2001 var Ã­bÃºafjÃ¶ldinn 363.987. ÃžessalÃ³nÃ­ka er Ã¶nnur efnahags-, iÃ°naÃ°ar-, viÃ°skipta- og stjÃ³rnmÃ¡lamiÃ°ja Grikklands.'}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYmElz46k7_E"
      },
      "source": [
        "Next, we load the previously trained `ByteLevelBPETokenizer` tokenizer to pre-process the raw text data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "lySwpeYVc_Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdd7144-a350-41f9-fcb0-1cce6f90f288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnEFmLLylnQb"
      },
      "source": [
        "We can then write the function that will preprocess the raw text data. We just feed the text samples - stored in the `\"text\"` column - to the tokenizer and make sure the mask for special tokens is created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "wcpWIxX8dIAO"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lco7GkZ8nF-a"
      },
      "source": [
        "and apply the tokenization function to every text sample via the convenient `map(...)` function of Datasets. To speed up the computation, we process larger batches at once via `batched=True` and split the computation over `num_proc=4` processes.\n",
        "\n",
        "**Note**: Running this command on the whole dataset might take up to 10 minutes â˜•."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "h6cjpFO2dTYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc78f78a-efd2-4632-ca34-516fd60c07dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-233b6f300b5cb0a7.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-4154dd4be7e3a4aa.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-255feeacd545375e.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-54bb5d1670d4649e.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-69c421fdc9ea1816.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-0d2a0f66e6ac2025.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-2dd704cc7b1676ca.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-25705023eac542ca.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=raw_dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_E0jsY9onEf"
      },
      "source": [
        "The model can process the training data most efficiently if all data samples are of the same length. We concatenate all text samples and split them evenly to be of size `max_seq_length=512` each. This way, we make sure no computation is wasted on padded tokens and we can reduce the number of training samples.\n",
        "Causal Language modeling simply consists of predicting the next token which means that the labels are essentially the inputs just shifted to the left. Thus, we copy the `input_ids` tensor and set it to `labels`.\n",
        "\n",
        "Let's define such a function to group the dataset into equally sized data samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "HO_neGynddat"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // max_seq_length) * max_seq_length\n",
        "    result = {\n",
        "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR46Vvpwr6e5"
      },
      "source": [
        "We pass `group_texts` to the `map(...)` function and set `batched=True` to make sure that the function is applied to a large batch of data samples. \n",
        "\n",
        "**Note**: Running this function on the whole dataset might take up to 50 minutes ðŸ•’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "UmzNAUVediDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a654e4-fc81-476d-ffa2-bbdbbb54aada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-8454875aab8edd35.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-039c7d5c03cadbb4.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-5065ad0e43314d73.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-eec988777927a10c.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-cc66255b54bd2da9.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-0aa21915a6f2df09.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-4ec2db898cc6f20b.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_is/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/cache-8a9e903d16ac141c.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jid2JqXOsVfR"
      },
      "source": [
        "Awesome, the data is now fully pre-processed and ready to be used for training ðŸ˜Ž."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_datasets['train'])"
      ],
      "metadata": {
        "id": "CA1CRLRqDzi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5b913c-6e98-4574-8a85-5a686850b5e3"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17643"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRvfr609LzWu"
      },
      "source": [
        "## 4. Pre-Training the model\n",
        "\n",
        "Now we will see how the power of Google's tensor processing unit (TPU) can be leveraged with Flax/JAX for the compute-intensive pre-training of language models.\n",
        "\n",
        "We need to import `jax`, `flax`, `optax`, `numpy` to define our training loop. Additionally, we make use of `tqdm` to better visualize the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "5qOhue4Xm1TO"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import flax\n",
        "import jax.numpy as jnp\n",
        "import math\n",
        "\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MGleTRG6Vor"
      },
      "source": [
        "At first, we define all relevant hyper-parameters for pretraining in this notebook:\n",
        "\n",
        "- Each TPU will process a batch size of `16`\n",
        "- The model is trained for `10` epochs\n",
        "- The learning rate starts at `3e-4` and is successfully linearly decayed with each training step\n",
        "- To reproduce the training run, a random seed is set to `0`.\n",
        "\n",
        "We can deduce the total batch size over all devices as well as the total number of training steps accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "y8lsJQy8liud"
      },
      "outputs": [],
      "source": [
        "per_device_batch_size = 16\n",
        "num_epochs = 10\n",
        "training_seed = 0\n",
        "learning_rate = 3e-4\n",
        "\n",
        "total_batch_size = per_device_batch_size * jax.device_count()\n",
        "num_train_steps = len(tokenized_datasets[\"train\"]) // total_batch_size * num_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9bRDBq5j3r"
      },
      "source": [
        "In the [official GPT2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) a batch size of 512 is used.\n",
        "\n",
        "Here, we use a batch size of `8 * 16 = 128` due to the TPU memory constraints of this notebook. When running this script locally on a TPUv3-8, one can easily use batch sizes of up to `8 * 64 = 512`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Tylp115u1r"
      },
      "source": [
        "Now we randomly initialized a `distilgpt2` model according to its configuration. To save memory and improve speed, we initialize the weights directly in `bfloat16` by setting `dtype=jnp.dtype(\"bfloat16\")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "aVr9TCzfacLN"
      },
      "outputs": [],
      "source": [
        "from transformers import FlaxAutoModelForCausalLM\n",
        "\n",
        "model = FlaxAutoModelForCausalLM.from_config(config, seed=training_seed, dtype=jnp.dtype(\"bfloat16\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMS_QkT76Lgk"
      },
      "source": [
        "Next, we define the learning rate schedule. A simple and effective learning rate schedule is the linear decay with warmup (click [here](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) for more information). For simplicity, we set the number of warmup steps simply to 0 here. The schedule is then fully defined by the number of training steps and the learning rate.\n",
        "\n",
        "It is recommended to use the [**optax**](https://github.com/deepmind/optax) library for training utilities, *e.g.* learning rate schedules and optimizers.\n",
        "\n",
        "To see how to define a learning rate schedule with warmup, please take a look at the [official Flax CLM pre-training script](https://github.com/huggingface/transformers/blob/master/examples/flax/language-modeling/run_clm_flax.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "kfBkuV1ck4rq"
      },
      "outputs": [],
      "source": [
        "linear_decay_lr_schedule_fn = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0yNxeU79F2"
      },
      "source": [
        "We will be using the standard Adam optimizer with weight decay, called AdamW (Adam + weight decay). \n",
        "\n",
        "AdamW can easily be imported from [optax](https://github.com/deepmind/optax) and is created from the just defined learning rate schedule as well as a couple of other hyper-parameters (*beta1*, *beta2*, *epsilon*) that are hard-coded in this notebook.\n",
        "\n",
        "For more information on AdamW (Adam + weight decay), one can take a look at [this](https://www.fast.ai/2018/07/02/adam-weight-decay/) blog post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "xRtpv_iamZd2"
      },
      "outputs": [],
      "source": [
        "adamw = optax.adamw(learning_rate=linear_decay_lr_schedule_fn, b1=0.9, b2=0.98, eps=1e-8, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g_fEbV-72Hc"
      },
      "source": [
        "Next, we will create the *training state* that includes the optimizer, the loss function, and is responsible for updating the model's parameters during training.\n",
        "\n",
        "Most JAX transformations (notably [jax.jit](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)) require functions that are transformed to have no side effects. This is because any such side-effects will only be executed once when the Python version of the function is run during compilation (see [Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html)). As a consequence, Flax models (which can be transformed by JAX transformations) are **immutable**, and the state of the model (i.e., its weight parameters) is stored *outside* of the model instance.\n",
        "\n",
        "Models are initialized and updated in a purely functional way: you pass the state to the model when calling it, and the model returns the new (possibly modified) state, leaving the model instance itself unchanged.\n",
        "\n",
        "Flax provides a convenience class [`flax.training.train_state.TrainState`](https://github.com/google/flax/blob/9da95cdd12591f42d2cd4c17089861bff7e43cc5/flax/training/train_state.py#L22), which stores things such as the model parameters, the loss function, the optimizer, and exposes an `apply_gradients` function to update the model's weight parameters.\n",
        "\n",
        "Alright, let's begin by defining our *training state* class. We create a `TrainState` class that stores the model's forward pass as the `apply_fn`, the `params`, and the AdamW optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "JHYfR67AoKRc"
      },
      "outputs": [],
      "source": [
        "state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYCejDd81TX"
      },
      "source": [
        "Next, let's implement a data loader for both training and evaluation.\n",
        "The data loader can be defined as a [Python generator](https://wiki.python.org/moin/Generators) that returns a batch model input every time it is called.\n",
        "\n",
        "First, a random permutation of the whole dataset is defined. \n",
        "Then, every time the training data collator is called the next batch of the randomized dataset is extracted, converted to a JAX array and sharded over all local TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Aos9GltTb3Ve"
      },
      "outputs": [],
      "source": [
        "def data_loader(rng, dataset, batch_size, shuffle=False):\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "    else:\n",
        "        batch_idx = jnp.arange(len(dataset))\n",
        "\n",
        "    batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7uoTXDLUzb-"
      },
      "source": [
        "At each training epoch, the dataset should be shuffled and superfluous samples that make the dataset not evenly divisible by the batch size are thrown away. Instead of passing the dataset, we prepare the indices of data samples to be used for both each training epoch. \n",
        "The indices for the training dataset are additionally randomly shuffled before each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU6idLb29xYu"
      },
      "source": [
        "During fine-tuning, we want to update the model parameters and evaluate the performance after each epoch. \n",
        "\n",
        "Let's write the functions `train_step` and `eval_step` accordingly. During training the weight parameters should be updated as follows:\n",
        "\n",
        "1. Define a loss function `loss_function` that first runs a forward pass of the model given data input. Remember that Flax models are immutable, and we explicitly pass it the state (in this case the model parameters and the RNG). `loss_function` returns a scalar loss (using the previously defined `state.loss_function`) between the model output and input targets.\n",
        "2. Differentiate this loss function using [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#evaluate-a-function-and-its-gradient-using-value-and-grad). This is a JAX transformation called [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), which computes the gradient of `loss_function` given the input to the function (i.e., the parameters of the model), and returns the value and the gradient in a pair `(loss, gradients)`.\n",
        "3. Compute the mean gradient over all devices using the collective operation [lax.pmean](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.pmean.html). As we will see below, each device runs `train_step` on a different batch of data, but by taking the mean here we ensure the model parameters are the same on all devices.\n",
        "4. Use `state.apply_gradients`, which applies the gradients to the weights.\n",
        "\n",
        "Below, you can see how each of the described steps above is put into practice.\n",
        "\n",
        "Also note that the `labels` are shifted one to the left and the last token of the `logits` is cut. This way, the model learns to predict the **next** token as defined in causal language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "GjKzb0zJd-aH"
      },
      "outputs": [],
      "source": [
        "def train_step(state, batch, dropout_rng):\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "        \n",
        "        loss = optax.softmax_cross_entropy(logits[..., :-1, :], onehot(labels[..., 1:], logits.shape[-1])).mean()\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    metrics = jax.lax.pmean(\n",
        "        {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n",
        "    )\n",
        "\n",
        "    return new_state, metrics, new_dropout_rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCPedI-B-FMQ"
      },
      "source": [
        "Now, we want to do parallelized training over all TPU devices. To do so, we use [`jax.pmap`](https://jax.readthedocs.io/en/latest/jax.html?highlight=pmap#parallelization-pmap). This will compile the function once and run the same program on each device (it is an [SPMD program](https://en.wikipedia.org/wiki/SPMD)). When calling this pmapped function, all inputs (`\"state\"`, `\"batch\"`, `\"dropout_rng\"`) should be replicated for all devices, which means that the first axis of each argument is used to map over all TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "w3k1Lqerpw5k"
      },
      "outputs": [],
      "source": [
        "parallel_train_step = jax.pmap(train_step, \"batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DWFAZM6A8uf"
      },
      "source": [
        "Similarly, we can now define the evaluation step. Here, the function is much easier as we don't need to compute any gradients. To better monitor the performance improvement during training, the next token loss is computed and stored in a `metric` dictionary during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "EGEv7dyfpW4p"
      },
      "outputs": [],
      "source": [
        "def eval_step(params, batch):\n",
        "    labels = batch.pop(\"labels\")\n",
        "\n",
        "    logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "    loss = optax.softmax_cross_entropy(logits[..., :-1, :], onehot(labels[..., 1:], logits.shape[-1])).mean()\n",
        "\n",
        "    # summarize metrics\n",
        "    metrics = {\"loss\": loss, \"perplexity\": jnp.exp(loss)}\n",
        "    metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guaYWTvFA_66"
      },
      "source": [
        "Similarly, we also apply `jax.pmap` to the evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "0B8U2r2RpzjV"
      },
      "outputs": [],
      "source": [
        "parallel_eval_step = jax.pmap(eval_step, \"batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLaM60PCY8Ka"
      },
      "source": [
        "Next, we replicate/copy the weight parameters on each device, so that we can pass them to our parallelized mapped functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "kncZTfALp3PG"
      },
      "outputs": [],
      "source": [
        "state = flax.jax_utils.replicate(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2xg8oI-ZJ3P"
      },
      "source": [
        "We can almost start training! In a final preparation step, we generate a seeded [**PRNGKey**](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax-random-prngkey) used as the random seed for dropout layers and dataset shuffling.\n",
        "\n",
        "Similar to how we had to copy/replicate the state on all 8 TPU devices, we also need to generate one `PRNGKey` per device, which is why we split the initial `rng` key into 8 random seeds. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "idu3E9ubqZH3"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(training_seed)\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKuMWHicbede"
      },
      "source": [
        "Now, we are all set to finally start training! \n",
        "Let's put all the pieces together and write the training loop. \n",
        "\n",
        "We start each epoch by generating a new random seed that will be used for dataset shuffling, the dropout layers and the input token masking. \n",
        "\n",
        "Next, we generate the training dataset indices.\n",
        "In the first nested loop - the training loop - we shard the input batch on all 8 TPU devices, and run the training step. \n",
        "\n",
        "Analogs, in the second nested loop - the evaluation loop - the evaluation batches are sharded and the evaluation step is run.\n",
        "\n",
        "**Note**: It might seem that the following cell \"hangs\" when executed for the first time. This is because JAX first traces & compiles the code, the very first time it is run. After the first training step, you should notice that execution is much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "U946A-YZp-Pe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "81e9cc38f2f44ae8a885054d58c0ea8f",
            "aaafa46d30f847ddbb955d07c7892e8b",
            "2a9cc4d936514a36b7a140ffa16d6bab",
            "923e89c287884f0a883155bdace35fce",
            "e7520970360e42fd836f9734ccc1e170",
            "b9419274f0ce49288364b089857c592f",
            "80b5b325fd724a26922cbf2f02e2a5a4",
            "a145090d441349978b4ca216bbec67d0",
            "8799b4252a1d4c14be5ecc97b79a9c47",
            "10e56c87c886484f93e38db9c4480cd4",
            "e9dee16bf5004e438ec27baaeaf9a6f6",
            "ea02bfb42e624a8689be0645e9a9878c",
            "6a292ee94d2348789f7c52715c82fdb0",
            "cfc1c15a8dc64928bb2806cb679b21e6",
            "fe24320bd56b4992826d1c37c604b2a2",
            "2b6fadd7b46b4b4bafc1f5026bd6381a",
            "6ab2791be7e941b2876d4da2f79f1235",
            "5085793cdfbe4ef8994227637baf0c34",
            "9b84d2e2b9a44e2f9b0ed9065dbfd3d5",
            "a7ca873854f6450cadaeb3c9581582e7",
            "f500c50e1a6f4abfb8f81b5e36fd4683",
            "e0e3c8a33b1247ba8bd44a4c9f839c80"
          ]
        },
        "outputId": "d0a7ee29-8c55-4836-fc4b-88e9f0e84bbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch ...:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81e9cc38f2f44ae8a885054d58c0ea8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training...:   0%|          | 0/137 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea02bfb42e624a8689be0645e9a9878c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-95ef250df5bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# Model forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rngs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprogress_bar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m     \u001b[0mexecute\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m       \u001b[0mexecute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_pmap_impl_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2256\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_bind_continuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mxla_pmap_impl_lazy\u001b[0;34m(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *args)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_emap_apply_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m   \u001b[0mabstract_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m   compiled_fun, fingerprint = parallel_callable(\n\u001b[0m\u001b[1;32m    975\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_axis_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       \u001b[0min_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes_thunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_invars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mparallel_callable\u001b[0;34m(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *avals)\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_axis_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m       in_axes, out_axes_thunk, donated_invars, global_arg_shapes, avals)\n\u001b[0;32m-> 1228\u001b[0;31m   \u001b[0mpmap_executable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmap_computation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mWeakRefList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpmap_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsafe_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmap_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfingerprint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1509\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPmapExecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPmapExecutable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_hlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(xla_computation, pci, replicas, parts, shards, tuple_args, unordered_effects, ordered_effects, host_callbacks, keepalive)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     with dispatch.log_elapsed_time(\n\u001b[1;32m   1637\u001b[0m         f\"Finished XLA compilation of {pci.name} in {{elapsed_time}} sec\"):\n\u001b[0;32m-> 1638\u001b[0;31m       compiled = dispatch.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   1639\u001b[0m           pci.backend, xla_computation, compile_options, host_callbacks)\n\u001b[1;32m   1640\u001b[0m     handle_args = InputsHandler(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m   return backend_compile(backend, serialized_computation, compile_options,\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          host_callbacks)\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, built_c, options, host_callbacks)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m   \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;31m# TODO(phawkins): update users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in tqdm(range(1, num_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True):\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # -- Train --\n",
        "    train_loader = data_loader(input_rng, tokenized_datasets[\"train\"], total_batch_size, shuffle=True)\n",
        "    with tqdm(total=len(tokenized_datasets[\"train\"]) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
        "        for model_inputs in train_loader:\n",
        "            # Model forward\n",
        "            state, train_metric, dropout_rngs = parallel_train_step(state, model_inputs, dropout_rngs)\n",
        "\n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "        progress_bar_train.write(\n",
        "              f\"Train... ({epoch}/{num_epochs} | Loss: {round(train_metric['loss'].mean(), 3)}, Learning Rate: {round(train_metric['learning_rate'].mean(), 6)})\"\n",
        "        )\n",
        "\n",
        "    # -- Eval --\n",
        "    eval_loader = data_loader(input_rng, tokenized_datasets[\"validation\"], total_batch_size)\n",
        "    eval_metrics = []\n",
        "   \n",
        "    with tqdm(total=len(tokenized_datasets[\"validation\"]) // total_batch_size, desc=\"Evaluation...\", leave=False) as progress_bar_eval:\n",
        "        for model_inputs in eval_loader:\n",
        "            # Model forward\n",
        "            eval_metric = parallel_eval_step(state.params, model_inputs)\n",
        "            eval_metrics.append(eval_metric)\n",
        "\n",
        "            progress_bar_eval.update(1)\n",
        " \n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_map(jnp.mean, eval_metrics)\n",
        "        progress_bar_eval.write(\n",
        "            f\"Eval... ({epoch}/{num_epochs} | Loss: {eval_metrics['loss']} | Perplexity: {eval_metrics['perplexity']})\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI4XIhY-7hyh"
      },
      "source": [
        "It can be seen that in this colab training already reaches a speed of 2.42 training steps per second. Executing [**`run_clm_flax.py`**](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling/run_clm_flax.py) on a TPUv3-8 VM should be as fast as 7 training steps per second.\n",
        "\n",
        "For a more in-detail comparison of runtimes please refer to [this](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling#runtime-evaluation) table."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Values Dataset"
      ],
      "metadata": {
        "id": "lpGULv8Fvf7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in tokenized_datasets['train']:\n",
        "  break\n",
        "x.keys()"
      ],
      "metadata": {
        "id": "5QWdmS7iWhuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55bfb70-dd7a-4616-e3a3-3ec2d498f96d"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokenized_datasets['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu3lQ9W71aBx",
        "outputId": "00f5cf87-619c-4596-f28b-430d36823da4"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {'id': [[0], [1], [2]],\n",
        "           'name': ['mary', 'bob', 'eve'],\n",
        "           'age': [24, 53, 19]}\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(my_dict)\n",
        "for y in dataset:\n",
        "  break\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvPwzlIU155h",
        "outputId": "9bed3431-4688-4ab4-b7e9-64ed4139ab49"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': [0], 'name': 'mary', 'age': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_values():\n",
        "  for s2v in values_per_policy:\n",
        "    for s, v in s2v.items():\n",
        "      yield from v\n",
        "\n",
        "unique_values = list(np.unique(sorted(all_values())))\n",
        "unique_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FSDwAhuvX4K",
        "outputId": "41dbd8b4-fd4c-440a-923b-3f9296de58a1"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.0,\n",
              " -0.9,\n",
              " -0.81,\n",
              " 0.0,\n",
              " 0.5261265900000002,\n",
              " 0.5845851000000002,\n",
              " 0.6495390000000002,\n",
              " 0.7217100000000002,\n",
              " 0.8019000000000002,\n",
              " 0.8910000000000001,\n",
              " 0.9900000000000001,\n",
              " 1.1]"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(list(range(5)), 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V459LAIyA4p",
        "outputId": "36244c50-fc77-4f9b-e389-fc2f1a8f3ba3"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 1, 2, 0, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v[:10] for k, v in values_per_policy[-1].items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VR5ORNfqFAG",
        "outputId": "be9d34f6-3706-4be3-e01e-5fdaa0be2593"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 1): [0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.8019000000000002,\n",
              "  0.6495390000000002,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001],\n",
              " (1, 2): [0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.8019000000000002,\n",
              "  0.6495390000000002,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001,\n",
              "  0.9900000000000001],\n",
              " (2, 1): [0.7217100000000002,\n",
              "  0.6495390000000002,\n",
              "  0.5845851000000002,\n",
              "  0.5261265900000002,\n",
              "  0.47351393100000017,\n",
              "  0.42616253790000014,\n",
              "  0.38354628411000014,\n",
              "  0.3451916556990001,\n",
              "  0.31067249012910014,\n",
              "  0.2796052411161901],\n",
              " (0, 0): [0.7217100000000002,\n",
              "  0.5845851000000002,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.7217100000000002,\n",
              "  0.5845851000000002,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001],\n",
              " (0, 3): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " (2, 0): [0.7217100000000002,\n",
              "  0.5845851000000002,\n",
              "  0.47351393100000017,\n",
              "  0.7217100000000002,\n",
              "  0.7217100000000002,\n",
              "  0.5845851000000002,\n",
              "  0.47351393100000017,\n",
              "  0.7217100000000002,\n",
              "  0.7217100000000002,\n",
              "  0.7217100000000002],\n",
              " (2, 3): [-1.0,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.6495390000000002,\n",
              "  0.5261265900000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002],\n",
              " (0, 2): [1.1,\n",
              "  1.1,\n",
              "  1.1,\n",
              "  0.8910000000000001,\n",
              "  0.7217100000000002,\n",
              "  1.1,\n",
              "  1.1,\n",
              "  1.1,\n",
              "  1.1,\n",
              "  1.1],\n",
              " (2, 2): [0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.7217100000000002,\n",
              "  0.5845851000000002,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001,\n",
              "  0.8910000000000001],\n",
              " (1, 0): [0.6495390000000002,\n",
              "  0.5261265900000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.6495390000000002,\n",
              "  0.5261265900000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002,\n",
              "  0.8019000000000002],\n",
              " (1, 3): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "max_policy = 1\n",
        "\n",
        "input_ids = []\n",
        "labels = []\n",
        "attention_masks = []\n",
        "null = 0\n",
        "SHUFFLES_PER_PROMPT = 2\n",
        "\n",
        "def make_state_ids(state):\n",
        "  return list(1 + np.array(state))\n",
        "\n",
        "def make_value_ids(values):\n",
        "  return list(max(grid.height, grid.width) + np.array([unique_values.index(v) for v in values]))\n",
        "\n",
        "for i, s2v in enumerate(values_per_policy):\n",
        "  s2v_list = list(s2v.items())\n",
        "  for _ in range(len(s2v_list)):\n",
        "    *prompt, query = s2v_list\n",
        "\n",
        "    for seed in range(SHUFFLES_PER_PROMPT):\n",
        "      # prompt\n",
        "      new_input_ids = []\n",
        "      shuffled_prompt = random.sample(prompt, len(prompt))\n",
        "      for s, vs in shuffled_prompt:\n",
        "        new_input_ids += [*make_state_ids(s), *make_value_ids(vs)]\n",
        "  \n",
        "      # query\n",
        "      state, values = query\n",
        "      new_input_ids += make_state_ids(state)\n",
        "      query_value_ids = make_value_ids(values)\n",
        "  \n",
        "      # labels\n",
        "      new_labels = [null for _ in new_input_ids[:-1]] + query_value_ids\n",
        "      new_input_ids += query_value_ids[:-1]\n",
        "      input_ids.append(new_input_ids)\n",
        "      labels.append(new_labels)\n",
        "      attention_masks.append([1 for _ in new_labels])\n",
        "\n",
        "    # rotate s2v_list\n",
        "    s2v_list = [query, *prompt]\n",
        "\n",
        "  if i == max_policy:\n",
        "    break\n",
        "\n",
        "from datasets import Dataset\n",
        "values_dataset = Dataset.from_dict(dict(attention_masks=attention_masks, input_ids=input_ids, labels=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBY4kAoOuB2m",
        "outputId": "2da064db-775e-4fdb-ae2a-88c95f6c56bf"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@@@@@@@@@ ((1, 3), [0, 0, 0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 7, 2, 1, 7, 7, 7, 1, 2, 7, 6, 6, 3, 2, 7, 7, 7, 1, 4, 7, 7, 7, 1, 1, 7, 7, 7, 3, 1, 7, 7, 7, 2, 3, 4, 4, 4, 3, 3, 7, 7, 7, 1, 3, 5, 5, 5, 2, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 1, 7, 7, 7, 1, 1, 7, 7, 7, 3, 2, 7, 7, 7, 1, 3, 5, 5, 5, 2, 3, 4, 4, 4, 1, 2, 7, 6, 6, 3, 4, 7, 7, 7, 3, 3, 7, 7, 7, 1, 4, 7, 7, 7, 3, 1, 7, 7, 7, 2, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((1, 0), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 2, 7, 6, 6, 3, 3, 7, 7, 7, 1, 4, 7, 7, 7, 3, 2, 7, 7, 7, 1, 1, 7, 7, 7, 2, 4, 7, 7, 7, 2, 3, 4, 4, 4, 1, 3, 5, 5, 5, 3, 1, 7, 7, 7, 3, 4, 7, 7, 7, 2, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 5, 5, 5, 3, 4, 7, 7, 7, 3, 3, 7, 7, 7, 3, 1, 7, 7, 7, 1, 1, 7, 7, 7, 2, 3, 4, 4, 4, 2, 4, 7, 7, 7, 1, 4, 7, 7, 7, 3, 2, 7, 7, 7, 1, 2, 7, 6, 6, 2, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 2), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 4, 7, 7, 7, 2, 1, 7, 7, 7, 3, 1, 7, 7, 7, 1, 3, 5, 5, 5, 1, 2, 7, 6, 6, 2, 4, 7, 7, 7, 1, 1, 7, 7, 7, 2, 3, 4, 4, 4, 3, 4, 7, 7, 7, 3, 2, 7, 7, 7, 3, 3, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 5, 5, 5, 2, 3, 4, 4, 4, 3, 4, 7, 7, 7, 1, 4, 7, 7, 7, 2, 4, 7, 7, 7, 3, 2, 7, 7, 7, 3, 1, 7, 7, 7, 2, 1, 7, 7, 7, 1, 2, 7, 6, 6, 1, 1, 7, 7, 7, 3, 3, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 2), [-0.9, -0.9, -0.9])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 1, 7, 7, 7, 1, 4, 7, 7, 7, 2, 1, 7, 7, 7, 2, 4, 7, 7, 7, 2, 3, 4, 4, 4, 1, 1, 7, 7, 7, 3, 3, 7, 7, 7, 3, 2, 7, 7, 7, 3, 4, 7, 7, 7, 1, 2, 7, 6, 6, 1, 3, 5, 5]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 7, 3, 1, 7, 7, 7, 2, 3, 4, 4, 4, 1, 2, 7, 6, 6, 3, 3, 7, 7, 7, 1, 4, 7, 7, 7, 1, 1, 7, 7, 7, 2, 1, 7, 7, 7, 3, 2, 7, 7, 7, 2, 4, 7, 7, 7, 1, 3, 5, 5]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 3), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 1, 7, 7, 7, 2, 4, 7, 7, 7, 3, 1, 7, 7, 7, 1, 3, 5, 5, 5, 1, 2, 7, 6, 6, 1, 1, 7, 7, 7, 1, 4, 7, 7, 7, 3, 2, 7, 7, 7, 2, 3, 4, 4, 4, 3, 3, 7, 7, 7, 3, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 1, 7, 7, 7, 1, 3, 5, 5, 5, 3, 3, 7, 7, 7, 2, 3, 4, 4, 4, 1, 2, 7, 6, 6, 2, 4, 7, 7, 7, 3, 1, 7, 7, 7, 3, 2, 7, 7, 7, 1, 4, 7, 7, 7, 1, 1, 7, 7, 7, 3, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 0), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 4, 7, 7, 7, 2, 3, 4, 4, 4, 2, 4, 7, 7, 7, 2, 1, 7, 7, 7, 1, 1, 7, 7, 7, 3, 3, 7, 7, 7, 3, 4, 7, 7, 7, 1, 3, 5, 5, 5, 1, 2, 7, 6, 6, 3, 2, 7, 7, 7, 3, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 4, 7, 7, 7, 1, 2, 7, 6, 6, 2, 1, 7, 7, 7, 1, 3, 5, 5, 5, 3, 4, 7, 7, 7, 3, 3, 7, 7, 7, 1, 1, 7, 7, 7, 2, 4, 7, 7, 7, 3, 2, 7, 7, 7, 2, 3, 4, 4, 4, 3, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 3), [0, 0, 0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 5, 5, 5, 2, 4, 7, 7, 7, 1, 2, 7, 6, 6, 2, 3, 4, 4, 4, 3, 1, 7, 7, 7, 3, 2, 7, 7, 7, 3, 4, 7, 7, 7, 1, 1, 7, 7, 7, 2, 1, 7, 7, 7, 3, 3, 7, 7, 7, 1, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 1, 7, 7, 7, 3, 2, 7, 7, 7, 3, 4, 7, 7, 7, 3, 1, 7, 7, 7, 1, 2, 7, 6, 6, 2, 3, 4, 4, 4, 2, 4, 7, 7, 7, 2, 1, 7, 7, 7, 3, 3, 7, 7, 7, 1, 3, 5, 5, 5, 1, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 0), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 1, 7, 7, 7, 1, 4, 7, 7, 7, 2, 3, 4, 4, 4, 3, 2, 7, 7, 7, 2, 4, 7, 7, 7, 1, 2, 7, 6, 6, 1, 3, 5, 5, 5, 3, 3, 7, 7, 7, 3, 4, 7, 7, 7, 2, 1, 7, 7, 7, 1, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 3, 4, 4, 4, 2, 4, 7, 7, 7, 3, 1, 7, 7, 7, 3, 2, 7, 7, 7, 3, 4, 7, 7, 7, 1, 4, 7, 7, 7, 1, 2, 7, 6, 6, 3, 3, 7, 7, 7, 1, 3, 5, 5, 5, 2, 1, 7, 7, 7, 1, 1, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 1), [0.0, 0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 1, 7, 7, 7, 3, 1, 7, 7, 7, 1, 1, 7, 7, 7, 1, 3, 5, 5, 5, 3, 4, 7, 7, 7, 2, 4, 7, 7, 7, 2, 3, 4, 4, 4, 1, 2, 7, 6, 6, 1, 4, 7, 7, 7, 3, 3, 7, 7, 7, 3, 2, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 5, 5, 5, 3, 3, 7, 7, 7, 2, 4, 7, 7, 7, 2, 1, 7, 7, 7, 2, 3, 4, 4, 4, 1, 4, 7, 7, 7, 1, 2, 7, 6, 6, 1, 1, 7, 7, 7, 3, 1, 7, 7, 7, 3, 4, 7, 7, 7, 3, 2, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((1, 2), [-1.0, -1.0, -1.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 5, 5, 5, 2, 4, 7, 7, 7, 3, 4, 7, 7, 7, 3, 2, 7, 7, 7, 2, 1, 7, 7, 7, 3, 1, 7, 7, 7, 1, 1, 7, 7, 7, 3, 3, 7, 7, 7, 1, 2, 7, 6, 6, 1, 4, 7, 7, 7, 2, 3, 4, 4]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 2, 7, 6, 6, 2, 4, 7, 7, 7, 3, 4, 7, 7, 7, 1, 3, 5, 5, 5, 1, 4, 7, 7, 7, 3, 1, 7, 7, 7, 3, 3, 7, 7, 7, 3, 2, 7, 7, 7, 2, 1, 7, 7, 7, 1, 1, 7, 7, 7, 2, 3, 4, 4]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 1), [0.0, -0.81, -0.81])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 2, 7, 7, 7, 3, 4, 7, 7, 7, 2, 3, 4, 4, 4, 3, 1, 7, 7, 7, 1, 1, 7, 7, 7, 2, 4, 7, 7, 7, 1, 3, 5, 5, 5, 1, 4, 7, 7, 7, 2, 1, 7, 7, 7, 3, 3, 7, 7, 7, 1, 2, 7, 6]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 6]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 7, 1, 1, 7, 7, 7, 3, 1, 7, 7, 7, 1, 4, 7, 7, 7, 2, 4, 7, 7, 7, 1, 3, 5, 5, 5, 2, 3, 4, 4, 4, 3, 3, 7, 7, 7, 2, 1, 7, 7, 7, 3, 2, 7, 7, 7, 1, 2, 7, 6]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 6]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((1, 3), [0, 0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 2, 7, 7, 1, 1, 7, 7, 3, 3, 7, 7, 3, 1, 7, 7, 1, 3, 15, 15, 3, 4, 7, 7, 1, 4, 7, 7, 1, 2, 7, 7, 2, 3, 7, 7, 2, 1, 7, 7, 2, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 2, 7, 7, 3, 2, 7, 7, 1, 3, 15, 15, 2, 1, 7, 7, 3, 3, 7, 7, 2, 3, 7, 7, 1, 1, 7, 7, 3, 1, 7, 7, 1, 4, 7, 7, 3, 4, 7, 7, 2, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((1, 0), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 3, 4, 7, 7, 1, 4, 7, 7, 2, 3, 7, 7, 3, 2, 7, 7, 3, 3, 7, 7, 1, 3, 15, 15, 1, 1, 7, 7, 3, 1, 7, 7, 1, 2, 7, 7, 2, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 3, 1, 7, 7, 1, 3, 15, 15, 2, 3, 7, 7, 3, 3, 7, 7, 1, 2, 7, 7, 1, 4, 7, 7, 3, 4, 7, 7, 3, 2, 7, 7, 1, 1, 7, 7, 2, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 2), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 3, 7, 7, 2, 4, 7, 7, 1, 3, 15, 15, 1, 4, 7, 7, 1, 2, 7, 7, 1, 1, 7, 7, 3, 4, 7, 7, 2, 1, 7, 7, 3, 1, 7, 7, 3, 2, 7, 7, 3, 3, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 2, 1, 7, 7, 3, 2, 7, 7, 2, 3, 7, 7, 3, 4, 7, 7, 1, 2, 7, 7, 1, 3, 15, 15, 3, 1, 7, 7, 1, 4, 7, 7, 1, 1, 7, 7, 3, 3, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 2), [1.1, 1.1])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 2, 3, 7, 7, 3, 3, 7, 7, 1, 4, 7, 7, 1, 1, 7, 7, 2, 1, 7, 7, 3, 4, 7, 7, 1, 2, 7, 7, 3, 1, 7, 7, 3, 2, 7, 7, 1, 3, 15]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 3, 3, 7, 7, 1, 4, 7, 7, 3, 2, 7, 7, 1, 1, 7, 7, 1, 2, 7, 7, 2, 3, 7, 7, 2, 1, 7, 7, 3, 1, 7, 7, 2, 4, 7, 7, 1, 3, 15]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 3), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 2, 7, 7, 1, 2, 7, 7, 1, 3, 15, 15, 3, 3, 7, 7, 3, 1, 7, 7, 2, 1, 7, 7, 1, 4, 7, 7, 2, 3, 7, 7, 2, 4, 7, 7, 1, 1, 7, 7, 3, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 3, 7, 7, 1, 3, 15, 15, 1, 4, 7, 7, 2, 1, 7, 7, 3, 3, 7, 7, 3, 1, 7, 7, 3, 2, 7, 7, 1, 2, 7, 7, 2, 4, 7, 7, 1, 1, 7, 7, 3, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 0), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 1, 3, 15, 15, 3, 2, 7, 7, 1, 2, 7, 7, 1, 1, 7, 7, 3, 4, 7, 7, 3, 3, 7, 7, 1, 4, 7, 7, 2, 1, 7, 7, 2, 3, 7, 7, 3, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 3, 7, 7, 1, 2, 7, 7, 2, 4, 7, 7, 2, 1, 7, 7, 3, 4, 7, 7, 1, 1, 7, 7, 1, 4, 7, 7, 1, 3, 15, 15, 3, 3, 7, 7, 3, 2, 7, 7, 3, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 3), [0, 0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 2, 3, 7, 7, 1, 1, 7, 7, 2, 4, 7, 7, 3, 3, 7, 7, 1, 2, 7, 7, 3, 1, 7, 7, 3, 2, 7, 7, 1, 3, 15, 15, 2, 1, 7, 7, 1, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 15, 15, 1, 2, 7, 7, 3, 4, 7, 7, 3, 1, 7, 7, 3, 2, 7, 7, 2, 3, 7, 7, 2, 1, 7, 7, 2, 4, 7, 7, 3, 3, 7, 7, 1, 1, 7, 7, 1, 4, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 0), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 1, 7, 7, 2, 4, 7, 7, 3, 3, 7, 7, 1, 3, 15, 15, 2, 3, 7, 7, 1, 2, 7, 7, 3, 2, 7, 7, 2, 1, 7, 7, 1, 4, 7, 7, 3, 4, 7, 7, 1, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 3, 15, 15, 3, 3, 7, 7, 3, 4, 7, 7, 3, 2, 7, 7, 3, 1, 7, 7, 1, 2, 7, 7, 1, 4, 7, 7, 2, 4, 7, 7, 2, 3, 7, 7, 2, 1, 7, 7, 1, 1, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((2, 1), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 1, 4, 7, 7, 2, 4, 7, 7, 2, 3, 7, 7, 3, 1, 7, 7, 1, 2, 7, 7, 1, 1, 7, 7, 2, 1, 7, 7, 3, 3, 7, 7, 1, 3, 15, 15, 3, 2, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 4, 7, 7, 2, 4, 7, 7, 2, 1, 7, 7, 3, 1, 7, 7, 1, 3, 15, 15, 1, 4, 7, 7, 1, 1, 7, 7, 1, 2, 7, 7, 3, 3, 7, 7, 2, 3, 7, 7, 3, 2, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((1, 2), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[1, 2, 7, 7, 3, 3, 7, 7, 2, 4, 7, 7, 3, 4, 7, 7, 3, 1, 7, 7, 1, 4, 7, 7, 1, 3, 15, 15, 1, 1, 7, 7, 3, 2, 7, 7, 2, 1, 7, 7, 2, 3, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[3, 2, 7, 7, 1, 3, 15, 15, 2, 4, 7, 7, 3, 3, 7, 7, 1, 4, 7, 7, 1, 2, 7, 7, 1, 1, 7, 7, 3, 1, 7, 7, 3, 4, 7, 7, 2, 1, 7, 7, 2, 3, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "@@@@@@@@@ ((0, 1), [0.0, 0.0])\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 4, 7, 7, 3, 1, 7, 7, 3, 3, 7, 7, 2, 3, 7, 7, 1, 3, 15, 15, 2, 1, 7, 7, 3, 4, 7, 7, 1, 4, 7, 7, 1, 1, 7, 7, 3, 2, 7, 7, 1, 2, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n",
            "Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£Â£\n",
            "[2, 3, 7, 7, 3, 1, 7, 7, 2, 4, 7, 7, 1, 3, 15, 15, 1, 4, 7, 7, 1, 1, 7, 7, 3, 2, 7, 7, 3, 3, 7, 7, 3, 4, 7, 7, 2, 1, 7, 7, 1, 2, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7]\n",
            "â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬â‚¬\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for y in values_dataset:\n",
        "  break\n",
        "for v in y.values():\n",
        "  print(v)"
      ],
      "metadata": {
        "id": "NPgR0f-xYaDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c4d986-9984-4d25-cab3-4369156b2ca4"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[3, 4, 7, 7, 7, 2, 1, 7, 7, 7, 1, 2, 7, 6, 6, 3, 2, 7, 7, 7, 1, 4, 7, 7, 7, 1, 1, 7, 7, 7, 3, 1, 7, 7, 7, 2, 3, 4, 4, 4, 3, 3, 7, 7, 7, 1, 3, 5, 5, 5, 2, 4, 7, 7]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgEFu9ig5kl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b7794b008154e68ab15ebb0cef14c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bc9e822ac5421a8822625205b1aa6b",
              "IPY_MODEL_25ce075e4c744cae98bb5a56dd3bf524",
              "IPY_MODEL_0695fe0e13f74928b0de368c67e06168"
            ],
            "layout": "IPY_MODEL_30b76b9e3b274393916de5ce056c808f"
          }
        },
        "42bc9e822ac5421a8822625205b1aa6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_547d5df6121e4a4ab53dd772a51ff490",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f083aeb568c842ecbc5a844c3c944bce",
            "value": "100%"
          }
        },
        "25ce075e4c744cae98bb5a56dd3bf524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28cc97972ed04041a8d2b46a6f923c06",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c97ab44f12934247937016fd6bf2a4d4",
            "value": 1
          }
        },
        "0695fe0e13f74928b0de368c67e06168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e827d0ccc745d2a5e390c85ef11e73",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afe4e8752d5d413a9cee3f61e628ba41",
            "value": " 1/1 [00:01&lt;00:00,  1.92s/it]"
          }
        },
        "30b76b9e3b274393916de5ce056c808f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547d5df6121e4a4ab53dd772a51ff490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f083aeb568c842ecbc5a844c3c944bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28cc97972ed04041a8d2b46a6f923c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c97ab44f12934247937016fd6bf2a4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31e827d0ccc745d2a5e390c85ef11e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe4e8752d5d413a9cee3f61e628ba41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81e9cc38f2f44ae8a885054d58c0ea8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaafa46d30f847ddbb955d07c7892e8b",
              "IPY_MODEL_2a9cc4d936514a36b7a140ffa16d6bab",
              "IPY_MODEL_923e89c287884f0a883155bdace35fce"
            ],
            "layout": "IPY_MODEL_e7520970360e42fd836f9734ccc1e170"
          }
        },
        "aaafa46d30f847ddbb955d07c7892e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9419274f0ce49288364b089857c592f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_80b5b325fd724a26922cbf2f02e2a5a4",
            "value": "Epoch ...:   0%"
          }
        },
        "2a9cc4d936514a36b7a140ffa16d6bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a145090d441349978b4ca216bbec67d0",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8799b4252a1d4c14be5ecc97b79a9c47",
            "value": 0
          }
        },
        "923e89c287884f0a883155bdace35fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e56c87c886484f93e38db9c4480cd4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e9dee16bf5004e438ec27baaeaf9a6f6",
            "value": " 0/10 [01:25&lt;?, ?it/s]"
          }
        },
        "e7520970360e42fd836f9734ccc1e170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9419274f0ce49288364b089857c592f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b5b325fd724a26922cbf2f02e2a5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a145090d441349978b4ca216bbec67d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8799b4252a1d4c14be5ecc97b79a9c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10e56c87c886484f93e38db9c4480cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9dee16bf5004e438ec27baaeaf9a6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea02bfb42e624a8689be0645e9a9878c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a292ee94d2348789f7c52715c82fdb0",
              "IPY_MODEL_cfc1c15a8dc64928bb2806cb679b21e6",
              "IPY_MODEL_fe24320bd56b4992826d1c37c604b2a2"
            ],
            "layout": "IPY_MODEL_2b6fadd7b46b4b4bafc1f5026bd6381a"
          }
        },
        "6a292ee94d2348789f7c52715c82fdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab2791be7e941b2876d4da2f79f1235",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5085793cdfbe4ef8994227637baf0c34",
            "value": "Training...:   0%"
          }
        },
        "cfc1c15a8dc64928bb2806cb679b21e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b84d2e2b9a44e2f9b0ed9065dbfd3d5",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7ca873854f6450cadaeb3c9581582e7",
            "value": 0
          }
        },
        "fe24320bd56b4992826d1c37c604b2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f500c50e1a6f4abfb8f81b5e36fd4683",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e0e3c8a33b1247ba8bd44a4c9f839c80",
            "value": " 0/137 [01:25&lt;?, ?it/s]"
          }
        },
        "2b6fadd7b46b4b4bafc1f5026bd6381a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab2791be7e941b2876d4da2f79f1235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5085793cdfbe4ef8994227637baf0c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b84d2e2b9a44e2f9b0ed9065dbfd3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ca873854f6450cadaeb3c9581582e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f500c50e1a6f4abfb8f81b5e36fd4683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e3c8a33b1247ba8bd44a4c9f839c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}